{
    "chunks": [
        {
            "source": "LLMs.pdf",
            "page_number": 1,
            "text": "Introduction to LLMs (Large Language Models) Large Language Models (LLMs) are a breakthrough in artificial intelligence that have revolutionized how machines understand and generate human language. These models are capable of performing a wide range of tasks such as translation, summarization, question answering, and even creative writing \u2014 all by learning from massive text datasets. In this section, we will build a foundational understanding of what LLMs are, why they matter in data science, and how they differ from traditional machine learning models. What is an LLM? A Large Language Model is a type of AI model that uses deep learning, specifically transformer architectures, to process and generate natural language. These models are \u201clarge\u201d because they contain billions (or even trillions) of parameters \u2014 tunable weights that help the model make predictions. At their core, LLMs are trained to predict the next word in a sentence, given the words that came before. With enough data and training, they learn complex language patterns, world knowledge, and even reasoning skills. Why are LLMs Important? \u2022 Versatility: One LLM can perform dozens of tasks without needing task- specific training. \u2022 Zero-shot and few-shot learning: LLMs can handle tasks they\u2019ve never explicitly seen before, based on prompts or examples."
        },
        {
            "source": "LLMs.pdf",
            "page_number": 2,
            "text": "\u2022 Human-like generation: They produce text that is often indistinguishable from human writing. \u2022 Foundation for AI applications: They power modern tools like ChatGPT, Copilot, Bard, Claude, and more. How are LLMs Different from Traditional ML Models? Traditional ML Feature LLMs Models Input Structured data Natural language (text) General pretraining on large Training Task-specific text Thousands to Parameters Billions to trillions millions Adaptability Limited Highly adaptable via prompting Knowledge Feature- Implicit via word embeddings representation engineered Where are LLMs Used? LLMs are widely used across industries: \u2022 Customer support: Chatbots and automated help desks \u2022 Education: AI tutors, personalized learning \u2022 Healthcare: Clinical documentation and patient interaction \u2022 Software Development: Code generation and bug detection \u2022 Creative fields: Story writing, poetry, music lyrics"
        },
        {
            "source": "LLMs.pdf",
            "page_number": 3,
            "text": "History of LLMs Understanding the history of Large Language Models (LLMs) helps us appreciate how far we\u2019ve come in natural language processing (NLP) and the innovations that made today\u2019s AI systems possible. This section walks through the key milestones \u2014 from early statistical models to the modern transformer revolution. Early NLP Approaches Before LLMs, language tasks were handled using: \u2022 Rule-based systems: Manually written logic for grammar and syntax. \u2022 Statistical models: Such as n-gram models, which predicted the next word based on a fixed window of previous words. \u2022 Bag-of-words and TF-IDF: Used for basic text classification but ignored word order and context. These models worked for simple tasks, but failed to capture deeper meaning, semantics, or long-range dependencies in language. The Rise of Neural Networks With the rise of deep learning, models began learning richer representations: \u2022 Word Embeddings like Word2Vec (2013) and GloVe (2014) mapped words to continuous vector spaces. \u2022 Recurrent Neural Networks (RNNs) and LSTMs enabled models to process sequences, but they struggled with long texts and parallel processing."
        },
        {
            "source": "LLMs.pdf",
            "page_number": 4,
            "text": "Transformers: The Game Changer In 2017, Google introduced the Transformer architecture in the paper \u201cAttention is All You Need.\u201d Key features of transformers: \u2022 Self-attention mechanism allows the model to weigh the importance of different words, regardless of their position. \u2022 Enables parallelization, making training on massive datasets feasible. This led to a new generation of LLMs: Model Year Key Contribution BERT 2018 Bidirectional context understanding GPT-1 2018 Introduced unidirectional generation GPT-2 2019 Generated coherent long-form text T5 2020 Unified text-to-text framework 175B parameters, capable of few-shot GPT-3 2020 learning ChatGPT / GPT-3.5 / 2022\u2013 Conversational abilities, better GPT-4 2023 reasoning Claude, Gemini, LLaMA, Open-source and scalable 2023+ Mistral alternatives Pretraining & Finetuning The modern LLM pipeline consists of: 1. Pretraining on a large corpus of general text (e.g., books, Wikipedia, web pages). 2. Finetuning for specific tasks (e.g., summarization, coding help)."
        },
        {
            "source": "LLMs.pdf",
            "page_number": 5,
            "text": "3. Reinforcement Learning from Human Feedback (RLHF) \u2014 used to make models safer and more helpful (e.g., ChatGPT). Summary The evolution of LLMs is a story of scale, data, and architecture. The shift from handcrafted rules to deep neural transformers has allowed machines to understand and generate language with remarkable fluency."
        },
        {
            "source": "LLMs.pdf",
            "page_number": 6,
            "text": "How LLMs Work In this section, we break down the inner workings of Large Language Models (LLMs). While these models seem like magic from the outside, they are grounded in fundamental machine learning and deep learning principles \u2014 especially the transformer architecture. We\u2019ll go through how LLMs process text, represent meaning, and generate coherent outputs. Tokenization: Breaking Text into Units LLMs do not process text as raw strings. Instead, they break input text into smaller units called tokens. Tokens can be: \u2022 Whole words (for simple models) \u2022 Subwords (e.g., \u201cun\u201d + \u201cbeliev\u201d + \u201cable\u201d) \u2022 Characters (rare for LLMs, used in specific domains) This process helps reduce the vocabulary size and handle unknown or rare words efficiently. Popular tokenizers include Byte-Pair Encoding (BPE) and SentencePiece. Embeddings: Converting Tokens to Vectors Once text is tokenized, each token is mapped to a high-dimensional vector through an embedding layer. These embeddings capture relationships between words based on context. For example, the words \u201cking\u201d and \u201cqueen\u201d will be closer in the embedding space than unrelated words like \u201cbanana\u201d or \u201ccar\u201d."
        },
        {
            "source": "LLMs.pdf",
            "page_number": 7,
            "text": "The Transformer Architecture The core of LLMs is the transformer, introduced in 2017. It replaced earlier models like RNNs and LSTMs by allowing for better performance and scalability. Key components of a transformer: \u2022 Self-Attention Mechanism: Enables the model to focus on different parts of the input when processing each token. For example, in the sentence \u201cThe cat sat on the mat,\u201d the word \u201csat\u201d may attend more to \u201ccat\u201d and \u201cmat\u201d than to \u201cthe\u201d. \u2022 Multi-Head Attention: Allows the model to capture different types of relationships simultaneously. \u2022 Feedforward Networks: Add depth and complexity to the model. \u2022 Positional Encoding: Since transformers process all tokens in parallel, they need a way to encode the order of tokens. These components are stacked in layers \u2014 more layers typically mean more modeling power. Training LLMs: Predicting the Next Token LLMs are trained using a simple but powerful objective: predict the next token given the previous tokens. For example: \u2022 Input: \u201cThe sun rises in the\u201d \u2022 Output: \u201ceast\u201d The model adjusts its internal weights using a large dataset and gradient descent to minimize prediction error. Over billions of examples, the model learns grammar, facts, reasoning patterns, and even basic common sense."
        },
        {
            "source": "LLMs.pdf",
            "page_number": 8,
            "text": "This process is known as causal language modeling in models like GPT. Other models like BERT use masked language modeling, where random tokens are hidden and the model must predict them. Generation: Producing Human-like Text Once trained, the model can generate text by predicting one token at a time: 1. Start with an input prompt. 2. Predict the next token based on context. 3. Append the new token to the prompt. 4. Repeat until a stopping condition is met. Several sampling strategies control the output: \u2022 Greedy decoding: Always choose the most likely next token. \u2022 Beam search: Explore multiple token sequences in parallel. \u2022 Top-k / top-p sampling: Add randomness for more creative or diverse outputs. Limitations of LLMs Despite their capabilities, LLMs have limitations: \u2022 No true understanding: They learn patterns, not meaning. \u2022 Hallucinations: They can generate plausible but false information. \u2022 Bias: Trained on large web corpora, they can inherit societal biases. \u2022 Compute-intensive: Training and running LLMs requires significant hardware resources."
        },
        {
            "source": "LLMs.pdf",
            "page_number": 9,
            "text": "Introduction to RAG-based Systems As Large Language Models (LLMs) become central to modern AI applications, a key limitation remains: they don\u2019t know anything beyond their training data. They cannot access up-to-date information or internal company documents unless explicitly provided. This is where RAG (Retrieval-Augmented Generation) systems come in. RAG bridges the gap between language generation and external knowledge, making LLMs more accurate, dynamic, and context-aware. What is a RAG System? Retrieval-Augmented Generation (RAG) is an AI architecture that combines: 1. A retriever \u2013 to search a knowledge base for relevant documents or facts. 2. A generator (LLM) \u2013 to synthesize a response using both the retrieved content and the input question. Rather than generating answers purely from internal memory (which may be outdated or incomplete), a RAG system fetches real documents and grounds the model\u2019s output in that information. Why Use RAG? RAG addresses several key challenges of LLMs: Problem in LLMs How RAG Helps Hallucination (made-up Anchors generation in real data facts)"
        },
        {
            "source": "LLMs.pdf",
            "page_number": 10,
            "text": "Problem in LLMs How RAG Helps Uses fresh, external sources like databases or Outdated knowledge websites Limited context window Dynamically injects only the most relevant info Domain-specific needs Connects model to private corpora, PDFs, etc. Basic Workflow of a RAG System 1. User query \u2192 2. Retriever fetches relevant documents (e.g., from a vector database) \u2192 3. Documents + query are passed to the LLM \u2192 4. LLM generates a grounded, accurate response. This loop allows the model to act more like a researcher with access to a searchable library. Components of a RAG Pipeline \u2022 Embedding Model: Converts text into dense vectors to enable similarity search. \u2022 Vector Store: A searchable index (e.g., FAISS, Weaviate, Pinecone) where document embeddings are stored. \u2022 Retriever: Queries the vector store with the input to find top-k most similar documents. \u2022 LLM: Uses the retrieved content and prompt to generate a final response. \u2022 Optional Reranker: Improves retrieval quality by reordering results."
        },
        {
            "source": "LLMs.pdf",
            "page_number": 11,
            "text": "Example Use Cases \u2022 Enterprise chatbots: Pull answers from internal documents and manuals. \u2022 Customer support: Query knowledge bases in real-time. \u2022 Academic research tools: Generate summaries grounded in actual papers. \u2022 Healthcare assistants: Retrieve clinical guidelines or patient history for personalized advice."
        }
    ]
}